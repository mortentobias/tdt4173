{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9641447,"sourceType":"datasetVersion","datasetId":5887591}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the stuff\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:50:41.254960Z","iopub.execute_input":"2024-10-16T14:50:41.255710Z","iopub.status.idle":"2024-10-16T14:50:41.261123Z","shell.execute_reply.started":"2024-10-16T14:50:41.255670Z","shell.execute_reply":"2024-10-16T14:50:41.260134Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# instantiate a distribution strategy\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.TPUStrategy(tpu)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:50:46.259547Z","iopub.execute_input":"2024-10-16T14:50:46.260444Z","iopub.status.idle":"2024-10-16T14:50:47.064621Z","shell.execute_reply.started":"2024-10-16T14:50:46.260401Z","shell.execute_reply":"2024-10-16T14:50:47.063127Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# detect and init the TPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tpu \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPUClusterResolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# instantiate a distribution strategy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39mtpu\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39minitialize_tpu_system(tpu)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py:235\u001b[0m, in \u001b[0;36mTPUClusterResolver.__init__\u001b[0;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new TPUClusterResolver object.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mThe ClusterResolver will then use the parameters to query the Cloud TPU APIs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Google Cloud environment.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# Default Cloud environment\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud_tpu_client \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m      \u001b[49m\u001b[43mzone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m      \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m      \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdiscovery_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscovery_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud_tpu_client\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m   \u001b[38;5;66;03m# Directly connected TPU environment\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cloud_tpu_client/client.py:139\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    136\u001b[0m tpu \u001b[38;5;241m=\u001b[39m _get_tpu_name(tpu)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease provide a TPU Name to connect to.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tpu \u001b[38;5;241m=\u001b[39m _as_text(tpu)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tpu\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrpc://\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Please provide a TPU Name to connect to."],"ename":"ValueError","evalue":"Please provide a TPU Name to connect to.","output_type":"error"}]},{"cell_type":"code","source":"# Define paths\nais_test_path = \"/kaggle/input/ais-vessels-ports-schedules/ais_test.csv\"\nais_train_path = \"/kaggle/input/ais-vessels-ports-schedules/ais_train.csv\"\nports_path = \"/kaggle/input/ais-vessels-ports-schedules/ports.csv\"\nschedules_path = \"/kaggle/input/ais-vessels-ports-schedules/schedules_to_may_2024.csv\"\nvessels_path = \"/kaggle/input/ais-vessels-ports-schedules/vessels.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Load and Preprocess Data\n\n\"\"\"\n# Read ais_train.csv\nais_train = pd.read_csv(ais_train_path, sep='|')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Temporal features\nais_train['time'] = pd.to_datetime(ais_train['time'])\nais_train['elapsed_time'] = (ais_train['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\nais_train['day_of_week'] = ais_train['time'].dt.dayofweek  # Monday=0, Sunday=6\nais_train['hour_of_day'] = ais_train['time'].dt.hour\nais_train = pd.get_dummies(ais_train, columns=['day_of_week', 'hour_of_day'], drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter out unrealistic speeds\nais_train = ais_train[ais_train['sog'] < 25]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map 'navstat' values\nais_train['navstat'] = ais_train['navstat'].replace(8, 0)  # Under way sailing -> Under way using engine\nais_train = ais_train[~((ais_train['navstat'].isin([1, 5])) & (ais_train['sog'] > 0))]\nais_train = ais_train[~((ais_train['navstat'] == 2) & (ais_train['sog'] > 5))]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode 'navstat'\nais_train = pd.get_dummies(ais_train, columns=['navstat'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Merge with vessel data\nvessels = pd.read_csv(vessels_path, sep='|')[['shippingLineId', 'vesselId']]\nvessels['new_id'] = range(len(vessels))\nvessel_id_to_new_id = dict(zip(vessels['vesselId'], vessels['new_id']))\nais_train = pd.merge(ais_train, vessels, on='vesselId', how='left')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge port data\nports = pd.read_csv(ports_path, sep='|')[['portId', 'latitude', 'longitude']]\nports = ports.rename(columns={'latitude': 'port_latitude', 'longitude': 'port_longitude'})\nais_train = pd.merge(ais_train, ports, on='portId', how='left')\nais_train = ais_train[~ais_train['portId'].isnull()] # remove with null ports","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Haversine distance and bearing calculations\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    # Earth radius in nautical miles\n    R = 3440.065\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n    return 2 * R * np.arcsin(np.sqrt(a))\n\ndef calculate_bearing(lat1, lon1, lat2, lon2):\n    lat1_rad, lat2_rad = np.radians(lat1), np.radians(lat2)\n    diff_long = np.radians(lon2 - lon1)\n    x = np.sin(diff_long) * np.cos(lat2_rad)\n    y = np.cos(lat1_rad) * np.sin(lat2_rad) - (np.sin(lat1_rad) * np.cos(lat2_rad) * np.cos(diff_long))\n    initial_bearing = np.arctan2(x, y)\n    return (np.degrees(initial_bearing) + 360) % 360\n\n# Calculate features\nprint(\"distance to port calculation\")\nais_train['distance_to_port'] = haversine_distance(\n    ais_train['latitude'], ais_train['longitude'],\n    ais_train['port_latitude'], ais_train['port_longitude']\n)\nprint(\"bearing to port calculation\")\nais_train['bearing_to_port'] = calculate_bearing(\n    ais_train['latitude'], ais_train['longitude'],\n    ais_train['port_latitude'], ais_train['port_longitude']\n)\n\nprint(\"done calculating\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define input and target features\ninput_features = ['latitude', 'longitude', 'sog', 'cog', 'heading', 'elapsed_time']\ninput_features.extend([col for col in ais_train.columns if 'day_of_week_' in col])\ninput_features.extend([col for col in ais_train.columns if 'hour_of_day_' in col])\nnavstat_columns = [col for col in ais_train.columns if col.startswith('navstat_')]\ninput_features.extend(navstat_columns)\ninput_features.extend(['distance_to_port', 'bearing_to_port'])\ntarget_columns = ['latitude', 'longitude']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize scalers\nscaler_input = MinMaxScaler()\nscaler_output = MinMaxScaler()\n\n# Scale input and output features\ninput_data = scaler_input.fit_transform(ais_train[input_features])\noutput_data = scaler_output.fit_transform(ais_train[target_columns])\n\n# Add scaled features back to DataFrame\nais_train_scaled = ais_train.copy()\nais_train_scaled[input_features] = input_data\nais_train_scaled[target_columns] = output_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create sequences per vessel\ndef create_sequences_per_vessel(df, time_steps):\n    X, y = [], []\n    vessel_ids = df['vesselId'].unique()\n    for vessel_id in vessel_ids:\n        vessel_data = df[df['vesselId'] == vessel_id].sort_values('elapsed_time')\n        inputs = vessel_data[input_features].values\n        targets = vessel_data[target_columns].values\n        if len(inputs) < time_steps:\n            continue  # Skip sequences shorter than time_steps\n        for i in range(len(inputs) - time_steps):\n            X.append(inputs[i:i + time_steps])\n            y.append(targets[i + time_steps])\n    return np.array(X), np.array(y)\n\n# Create sequences\ntime_step = 10\nX, y = create_sequences_per_vessel(ais_train_scaled, time_step)\n\n# Split into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDefine the LSTM Model\n\"\"\"\n# Define the LSTM Model\nmodel = Sequential()\nmodel.add(LSTM(1024, return_sequences=True, input_shape=(time_step, X_train.shape[2])))\nmodel.add(LSTM(1024))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y_train.shape[1]))\n\n# Compile the model\nwith tpu_strategy.scope():\n    model.compile(optimizer='adam', loss='mean_squared_error')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTrain the LSTM Model\nTweak the epochs and batch_size to get the best results\n\"\"\"\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nmodel.fit(\n    X_train, y_train,\n    # Usually stops at 23\n    epochs=100,\n    batch_size=128,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping],\n    verbose=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Prepare Test Data and Make Predictions\n\"\"\"\n# Load test data\nais_test = pd.read_csv(\"ais_test.csv\")\nais_test['time'] = pd.to_datetime(ais_test['time'])\nais_test['elapsed_time'] = (ais_test['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\nais_test['new_id'] = ais_test['vesselId'].map(vessel_id_to_new_id)\n\nais_test['day_of_week'] = ais_test['time'].dt.dayofweek\nais_test['hour_of_day'] = ais_test['time'].dt.hour\n\n# One-hot encode\nais_test = pd.get_dummies(ais_test, columns=['day_of_week', 'hour_of_day'], drop_first=True)\n\n# Ensure all columns in ais_test match those in ais_train\nfor col in input_features:\n    if col not in ais_test.columns:\n        ais_test[col] = 0\n\n# One-hot encode 'navstat' in test data (if available)\n# If 'navstat' is not available in test data, you may need to handle this accordingly\n\n# Merge with last known positions from training data\n# Get the last 'time_step' records for each vessel from training data\nlast_positions = ais_train_scaled.groupby('vesselId').apply(lambda x: x.sort_values('elapsed_time').tail(time_step))\nlast_positions = last_positions.reset_index(drop=True)\n\n# Prepare sequences for each vessel in the test set\nvessel_sequences = {}\nfor vessel_id in ais_test['vesselId'].unique():\n    if vessel_id in last_positions['vesselId'].values:\n        vessel_data = last_positions[last_positions['vesselId'] == vessel_id]\n        seq = vessel_data[input_features].values\n        if len(seq) < time_step:\n            # Pad sequences if necessary\n            seq = np.pad(seq, ((time_step - len(seq), 0), (0, 0)), mode='constant')\n        vessel_sequences[vessel_id] = seq\n    else:\n        # If no data available, create a default sequence (e.g., zeros or mean values)\n        seq = np.zeros((time_step, len(input_features)))\n        vessel_sequences[vessel_id] = seq\n\n# Create test sequences\nX_test = []\nfor idx, row in ais_test.iterrows():\n    vessel_id = row['vesselId']\n    seq = vessel_sequences[vessel_id]\n    X_test.append(seq)\nX_test = np.array(X_test)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Inverse transform predictions\ny_pred_inverse = scaler_output.inverse_transform(y_pred)\n\n\"\"\"\n    Prepare Submission File\n\"\"\"\n\n# Prepare submission\nsubmission_df = pd.DataFrame({\n    'ID': ais_test['ID'].values,\n    'longitude_predicted': y_pred_inverse[:, target_columns.index('longitude')],\n    'latitude_predicted': y_pred_inverse[:, target_columns.index('latitude')]\n})\n\n# Ensure the submission file has the required columns\nsubmission_df = submission_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n\n# Save submission file\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\n# Display submission\nprint(submission_df.head())\nprint(f\"Submission DataFrame shape: {submission_df.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}